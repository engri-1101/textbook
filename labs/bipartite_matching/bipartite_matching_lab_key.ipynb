{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from networkx.algorithms import bipartite\n",
    "import networkx as nx\n",
    "from ortools.linear_solver import pywraplp as OR\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import copy\n",
    "import pickle\n",
    "from bokeh import palettes\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.tile_providers import get_provider, Vendors\n",
    "from bokeh.models import (GraphRenderer, Circle, MultiLine, StaticLayoutProvider,\n",
    "                          HoverTool, TapTool, EdgesAndLinkedNodes, NodesAndLinkedEdges,\n",
    "                          ColumnDataSource, LabelSet, NodesOnly)\n",
    "from bipartite_matching import *\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Routing Bipartite Matching Lab\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* Construct the taxi-routing problem as a bipartite graph.\n",
    "* Use the maximum cardinality matching problem to solve the taxi-routing problem.\n",
    "* Compare the optimal solution to the taxi-routing problem with the original routing.\n",
    "    \n",
    "<font color='red'> **Instructor Comments** </font>\n",
    "\n",
    "<font color='blue'> **Solutions** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bipartite Graph Formulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the taxi trips information as well as NYC street nodes and arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = pd.read_csv('data/2013-09-01_trip_data_manhattan.csv').drop(columns='id')\n",
    "nodes_df = pd.read_csv('data/nyc_nodes_manhattan.csv').drop(columns='Unnamed: 0')\n",
    "arcs_df = pd.read_csv('data/nyc_links_manhattan.csv').drop(columns='Unnamed: 0')\n",
    "times_df = pd.read_csv('data/times.csv', index_col =0)\n",
    "times_df.columns = times_df.columns.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at an example input. Each trip consists of the unique vehicle id (medallion), location id of the pickup node and drop-off node, pickup time and drop-off time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of example trip_ids\n",
    "ex_trips = [68326, 69501, 70802, 68619, 69802, 70142, 68751, 69558, 70296, 68272]\n",
    "# Locate the corresponding trip information\n",
    "trips = trips_df.iloc[ex_trips]\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a bipartite graph $G = (D, P, E)$ where $D$ and $P$ are disjoint sets corresponding to the \"left\" and \"right\" sides of the graph, and $E$ is the set of edges, each of which has exactly one endpoint in $D$ and one endpoint in $P$.\n",
    "\n",
    "Let $D = \\{(d_1, T^d_{1}),(d_2, T^d_{2}),\\ldots,(d_n, T^d_{n})\\}$ denote the set of drop-off nodes, $P = \\{(p_1, T^p_{1}),(p_2, T^p_{2}),\\ldots,(p_n, T^p_{n})\\}$ denote the set of pickup nodes, where there are $n$ taxi trips to be covered. We can express the set of taxi trips $N = \\{(p_i, T^p_{i}, d_i, T^d_{i}): i\\in \\{1,\\ldots,n\\}\\}$; that is, the taxi history shows that the $i$th trip picks up a passenger at time $T^p_{i}$ at point $p_i$ and drops off the fare at time $T^d_{i}$ at point $d_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct nodes and edges for the bipartite graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize nodes and edges\n",
    "DO_nodes = list()\n",
    "PU_nodes = list()\n",
    "edges = list()\n",
    "# Initialize a dict that maps a PU node to a DO node\n",
    "PUtoDO = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node is a tuple of (location_id, time, trip_id, \"DO\"/\"PU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in trips.iterrows():\n",
    "    s = row['start_node']\n",
    "    t = row['end_node']\n",
    "    s_t = row['start_time']\n",
    "    t_t = s_t + row['trip_time']\n",
    "    DO_node = (int(t), t_t, index, 'DO')\n",
    "    PU_node = (int(s), s_t, index, 'PU')\n",
    "    DO_nodes.append(DO_node)\n",
    "    PU_nodes.append(PU_node)\n",
    "    PUtoDO[PU_node] = DO_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Assume that a taxi handles two consecutive trips $i$ and $j$ (in order), where the drop-off time at location $d_i$ is $T^d_i$ and the pickup time at location $p_j$ is $T^p_j$. What is the *elapsed time* between the drop-off time of trip $i$ and the pickup time of trip $j$ (express in terms of $T^d_i$ and $T^p_j$) ?\n",
    "\n",
    "**A:** <font color='blue'> The *elapsed time* is $T^p_j - T^d_i$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Assume that $time(d_i, p_j)$ is the time needed to travel between location point $d_i$ and location point $p_j$. What relationship must hold between the *elasped time* between the drop-off time of trip $i$ and the pickup time of trip $j$ and $time(d_i, p_j)$ in order for a taxi to cover both trip $i$ and trip $j$ (express in terms of $time(d_i, p_j), T^d_i$ and $T^p_j$)? What does this inequality mean?\n",
    "\n",
    "**A:** <font color='blue'> $time(d_i, p_j) \\leq T^p_j - T^d_i$. It means that a taxi can reach the new pickup at $p_j$ in time after dropping off the fare at $d_i$. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** In the previous question, what if the taxi can only stay for a maximum of $\\delta$ minutes? What is the new inequality and what does it mean?\n",
    "\n",
    "**A:** <font color='blue'> $T^p_j - T^d_i - \\delta \\leq time(d_i, p_j) \\leq T^p_j - T^d_i$. The new inequality means that a taxi can reach the new pickup at $p_j$ in time after dropping off the fare at $d_i$, and also doesn’t need to wait more than $\\delta$ minutes for the new pickup to be ready. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Based on the new inequality you just estabilished, can you define the set $E$ mathematically?\n",
    "\n",
    "**A:** <font color='blue'> $$E = \\{\\{(d_i, T^d_{i}),(p_j, T^p_{j})\\}: T^p_{j} - T^d_{i}-\\delta \\leq time(d_i, p_j) \\leq T^p_{j} - T^d_{i}\\}$$ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the nodes by time\n",
    "DO_nodes = sorted(DO_nodes, key = lambda x: x[1])\n",
    "PU_nodes = sorted(PU_nodes, key = lambda x: x[1])\n",
    "\n",
    "# Specify the edges\n",
    "max_waiting_time = 10 # delta\n",
    "\n",
    "for DO_node in DO_nodes:\n",
    "    for PU_node in PU_nodes:\n",
    "        if PU_node[1] > DO_node[1] + max_waiting_time:\n",
    "            break\n",
    "        else:\n",
    "            if PU_node[1] >= DO_node[1]:\n",
    "                time = times_df.at[(DO_node[0], PU_node[0])]\n",
    "                if ((PU_node[1] - DO_node[1]) - max_waiting_time  <= time) & (time <= (PU_node[1] - DO_node[1])):\n",
    "                    edges.append((DO_node, PU_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "B.add_nodes_from(DO_nodes, bipartite=0)\n",
    "B.add_nodes_from(PU_nodes, bipartite=1)\n",
    "# Add edges only between nodes of opposite node sets\n",
    "B.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**\n",
    "\n",
    "If a maximum cardinality matching is of size $m$, then the minimum number of taxis needed to cover all $n$ trips is $n-m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = nx.bipartite.maximum_matching(B, DO_nodes)\n",
    "print('Size of max cardinality matching:', int(len(match)/2)) # divided by two because the output edges are directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What is the minimum number of taxis needed to cover all the trips according to the size of the maximum cardinality matching?\n",
    "\n",
    "**A:** <font color='blue'> 4 taxis.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the information given by maximum cardinality matching to track down the optimal taxi trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_paths = match_to_path(match, trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ex_bipartite(B, match, opt_paths, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Interpret one optimal taxi trajectory that contains three trips based on the graph above using the location ids (remember each node is a tuple of (location_id, time, trip_id, \"DO\"/\"PU\")).\n",
    "\n",
    "**A:** <font color='blue'> 1447->1762->1163->24->79->931 (or 2119->275->1494->1809->20->1323)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the corresponding taxi paths on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = street_network(nodes_df, arcs_df, weight = 'trip_time')\n",
    "plot_taxi_route(G, opt_paths, nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bipartite Graph Formulation (At Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of selecting a sample selection of taxi trips, try filter the trips by time window of interest. The following example selects all the trips from 5 pm to 5:15 pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter trips by time window of interest\n",
    "start_time = 1020\n",
    "end_time = 1035\n",
    "trips = trips_df.copy()\n",
    "trips = trips[(trips.start_time >= start_time) & \n",
    "              (trips.start_time + trips.trip_time <= end_time)].copy()\n",
    "trips.start_time = trips.start_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes and edges are defined similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize nodes and edges\n",
    "DO_nodes = list()\n",
    "PU_nodes = list()\n",
    "edges = list()\n",
    "# Initialize a dict that maps a PU node to a DO node\n",
    "PUtoDO = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify nodes - each node is a tuple of (location_id, time, trip_id, \"DO\"/\"PU\")\n",
    "for index, row in trips.iterrows():\n",
    "    s = row['start_node']\n",
    "    t = row['end_node']\n",
    "    s_t = row['start_time']\n",
    "    t_t = s_t + row['trip_time']\n",
    "    DO_node = (int(t), t_t, index, 'DO')\n",
    "    PU_node = (int(s), s_t, index, 'PU')\n",
    "    DO_nodes.append(DO_node)\n",
    "    PU_nodes.append(PU_node)\n",
    "    PUtoDO[PU_node] = DO_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the nodes by time\n",
    "DO_nodes = sorted(DO_nodes, key = lambda x: x[1])\n",
    "PU_nodes = sorted(PU_nodes, key = lambda x: x[1])\n",
    "\n",
    "# Specify edges\n",
    "max_waiting_time = 10\n",
    "\n",
    "for DO_node in DO_nodes:\n",
    "    for PU_node in PU_nodes:\n",
    "        if PU_node[1] > DO_node[1] + max_waiting_time:\n",
    "            break\n",
    "        else:\n",
    "            if PU_node[1] >= DO_node[1]:\n",
    "                time = times_df.at[(DO_node[0], PU_node[0])]\n",
    "                if ((PU_node[1] - DO_node[1]) - max_waiting_time  <= time) & (time <= (PU_node[1] - DO_node[1])):\n",
    "                    edges.append((DO_node, PU_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "B.add_nodes_from(DO_nodes, bipartite=0)\n",
    "B.add_nodes_from(PU_nodes, bipartite=1)\n",
    "# Add edges only between nodes of opposite node sets\n",
    "B.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = nx.bipartite.maximum_matching(B, DO_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of max cardinality matching:', len(match) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of unmatched nodes (minimum number of taxis needed)\n",
    "# TODO: Assign num_taxi with the minimum number of taxis needed\n",
    "# num_taxi = XXX\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "num_taxi = len(DO_nodes) - len(match)/2\n",
    "### END SOLUTION\n",
    "\n",
    "print('min number of taxis needed: ', num_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum matching problem can also be formulated via an IP formulation.\n",
    "$$\\begin{align*}\n",
    "\\max \\quad & \\sum_{e \\in E}x_e \\\\\n",
    "\\text{s.t.} \\quad &  \\sum_{e \\in \\delta(v)} x_e \\leq 1 \\quad \\forall v \\in P \\cup D & (1)\\\\\n",
    "\\quad & x_e \\in \\{0,1\\} \\quad \\forall e \\in E & (2)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where, $\\delta(v)$ is the set of edges incident on the vertex $v \\in P \\cup D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that stores the list of edges adjacent to node\n",
    "incident = dict()\n",
    "for v in (DO_nodes + PU_nodes):\n",
    "    incident[v] = [edge for edge in edges if v in edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = OR.Solver('taxi_bipartite', OR.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n",
    "# Decision variables\n",
    "x = {}  \n",
    "for e in B.edges:\n",
    "    x[e] = solver.IntVar(0, 1, ('(%s)' % str(e)))\n",
    "\n",
    "solver.Maximize(sum(x[e] for e in edges))\n",
    "\n",
    "for v in (DO_nodes + PU_nodes):\n",
    "    # TODO: Specify constraint (1)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    if v in incident.keys():\n",
    "        solver.Add(sum(x[e] for e in incident[v]) <= 1)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.Solve()\n",
    "print('Solution:')\n",
    "print('Objective value =', solver.Objective().Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check our answer by reducing the max matching problem to a max flow problem - the max flow value should be equal to the size of max cardinality matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(PU_nodes + DO_nodes + ['s'] + ['t'])\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1], capacity = 1)\n",
    "for DO_node in DO_nodes:\n",
    "    G.add_edge('s', DO_node, capacity = 1)\n",
    "for PU_node in PU_nodes:\n",
    "    G.add_edge(PU_node, 't', capacity = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_value, flow_dict = nx.maximum_flow(G, \"s\", \"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max flow value: ', flow_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another alogrithm to solve max flow\n",
    "from networkx.algorithms.flow import shortest_augmenting_path\n",
    "print('max flow value: ', nx.maximum_flow(G, \"s\", \"t\", flow_func=shortest_augmenting_path)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn the process of building a bipartite graph and solving the max cardinality matching into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_match(start_time, end_time, trips_df, max_waiting_time):\n",
    "    # filter trips by time window of interest\n",
    "    trips = trips_df.copy()\n",
    "    trips = trips[(trips.start_time >= start_time) & \n",
    "                  (trips.start_time + trips.trip_time <= end_time)].copy()\n",
    "    trips.start_time = trips.start_time - start_time\n",
    "\n",
    "    # create a dictionary to map the location pairs to trip time\n",
    "    loc_time = dict()\n",
    "    for index, row in arcs_df.iterrows():\n",
    "        i = row['start']\n",
    "        j = row['end']\n",
    "        delay = row['trip_time']\n",
    "        loc_time[(i, j)] = delay\n",
    "    # Intialize nodes and edges\n",
    "    DO_nodes = list()\n",
    "    PU_nodes = list()\n",
    "    edges = list()\n",
    "    # Initialize a dict that maps a PU node to a DO node\n",
    "    PUtoDO = dict()\n",
    "    # Specify nodes - each node is a tuple of (location_id, time, trip_id, \"DO\"/\"PU\")\n",
    "    for index, row in trips.iterrows():\n",
    "        s = row['start_node']\n",
    "        t = row['end_node']\n",
    "        s_t = row['start_time']\n",
    "        t_t = s_t + row['trip_time']\n",
    "        DO_node = (int(t), t_t, index, 'DO')\n",
    "        PU_node = (int(s), s_t, index, 'PU')\n",
    "        DO_nodes.append(DO_node)\n",
    "        PU_nodes.append(PU_node)\n",
    "        PUtoDO[PU_node] = DO_node\n",
    "    DO_nodes = sorted(DO_nodes, key = lambda x: x[1])\n",
    "    PU_nodes = sorted(PU_nodes, key = lambda x: x[1])\n",
    "    \n",
    "    # Specify edges\n",
    "    for DO_node in DO_nodes:\n",
    "        for PU_node in PU_nodes:\n",
    "            if PU_node[1] > DO_node[1] + max_waiting_time:\n",
    "                break\n",
    "            else:\n",
    "                if PU_node[1] >= DO_node[1]:\n",
    "                    time = times_df.at[(DO_node[0], PU_node[0])]\n",
    "                    if ((PU_node[1] - DO_node[1]) - max_waiting_time <= time) & (time <= (PU_node[1] - DO_node[1])):\n",
    "                        edges.append((DO_node, PU_node))\n",
    "    \n",
    "    # load the model\n",
    "    B = nx.Graph()\n",
    "    # Add nodes with the node attribute \"bipartite\"\n",
    "    B.add_nodes_from(DO_nodes, bipartite=0)\n",
    "    B.add_nodes_from(PU_nodes, bipartite=1)\n",
    "    # Add edges only between nodes of opposite node sets\n",
    "    B.add_edges_from(edges)\n",
    "    \n",
    "    top_nodes = {n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "    match = nx.bipartite.maximum_matching(B, top_nodes)\n",
    "    num_taxi = len(DO_nodes) - len(match)/2\n",
    "\n",
    "    return B, match, num_taxi, trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifteen_min_B, fifteen_min_match, fifteen_min_vnum_taxi, fifteen_min_trips = max_match(1020, 1035, trips_df, 10)\n",
    "print('max cardinality matching:', len(fifteen_min_match)/2)\n",
    "print('min number of taxis needed to cover all trips:', fifteen_min_vnum_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try expanding the time window to 5-6 pm and restricting the max waiting time to 5 mininutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the code below and change START_TIME, END_TIME, and MAX_WAIT_TIME.\n",
    "# one_hr_B, one_hr_match, one_hr_num_taxi, one_hr_trips =  = max_match(START_TIME, END_TIME, trips_df, MAX_WAIT_TIME)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "one_hr_B, one_hr_match, one_hr_num_taxi, one_hr_trips = max_match(1020, 1080, trips_df, 5)\n",
    "### END SOLUTION\n",
    "print('max cardinality matching:', len(one_hr_match)/2)\n",
    "print('min number of taxis needed to cover all trips:', one_hr_num_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare one day's taxi routing solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the result of a day's data with a max waiting time of 10 min\n",
    "with open('data/day_match.pkl', 'rb') as f:\n",
    "    match = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal taxi paths\n",
    "opt_paths = match_to_path(match, trips_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the street network\n",
    "G = street_network(nodes_df, arcs_df, weight = 'trip_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the routes with more than 1 trip\n",
    "opt_paths2 = []\n",
    "for path in opt_paths:\n",
    "    if len(path) > 1:\n",
    "        opt_paths2.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_taxi_route(G, opt_paths2[:5], nodes_df,'Optimal Sample Taxi Routes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the original paths of the taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_paths = get_og_path(trips_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the routes with more than 1 trip\n",
    "og_paths2 = []\n",
    "for path in og_paths:\n",
    "    if len(path) > 1:\n",
    "        og_paths2.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_taxi_route(G, og_paths2[:5], nodes_df,'Original Sample Taxi Routes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare statistics of the taxi routings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_stats = get_taxi_stats(opt_paths, trips_df)\n",
    "og_stats = get_taxi_stats(og_paths, trips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics for the original taxi routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_stats(og_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics for the optimal taxi routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_stats(opt_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(og_stats, opt_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What conclusions can you draw from the histograms above in terms of the difference between the original and the optimal taxi routings?\n",
    "\n",
    "**A:** <font color='blue'> Total trip time and empty trip time are greatly reduced. However, to offset the reductions in trip time, the number of trips and on-trip percentage are increased.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compare different taxi routings is to compare their distribution of taxis over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data = get_day_dist(og_paths, opt_paths, times_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_taxi_dist(day_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the estimated original and optimal number of circulating taxis on this example day shown above?\n",
    "\n",
    "**A:** <font color='blue'> Original number of circulating taxis: 7798; optimal number of circulating taxis: 1305.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Minimizing the number of taxis while accounting for the waiting time of the drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bipartite graphs we formulated, the edges actually contain information of the waiting time. By cleverly incorporating the waiting time information into the weights of the edges, we can include secondary objectives in addition to the primary objective of minimizing the number of taxis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate six example trips\n",
    "ex_trips = [68634, 68536, 68894, 69699, 69963, 70937]\n",
    "trips = trips_df.iloc[ex_trips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a base max card matching of the six example trips\n",
    "B, match, num_taxi, trips = max_match(0, 1440, trips, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve fair distribution of taxi trips, we want to minimize the maximum waiting time between any two trips. Minimizing the maximum waiting time is equivalent to maximizing the minimum edge wait (negative of waiting time).\n",
    "A max-min matching (or a bottleneck matching) problem can be constructed as an IP:\n",
    "     $$\\begin{align*}\n",
    "    \\max_E \\quad  \\min \\quad & \\{c_ex_e|e \\in E\\} \\\\\n",
    "    \\text{s.t.} \\quad &  \\sum_{e \\in \\delta(v)} x_e \\leq 1 \\quad \\forall v \\in P \\cup D & (1)\\\\\n",
    "    \\quad & x_e \\in \\{0,1\\} \\quad \\forall e \\in E & (2)\\\\\n",
    "    \\end{align*}$$\n",
    "\n",
    "where, $\\delta(v)$ is the set of edges incident on the vertex $v \\in P \\cup D$, and $c_e$ is the edge weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly solving an IPA, we can come up with a bisection search algorithm below to find this minimized maximum (or a bottleneck) waiting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection_search(base_graph, base_match):\n",
    "    '''A bisection search algorithm that finds a max card matching with bottleneck waiting time'''\n",
    "    max_wait_times = []\n",
    "    for edge in base_graph.edges(data = True):\n",
    "        waiting_time = edge[1][1] - edge[0][1] - times_df.at[(edge[0][0], edge[1][0])]\n",
    "        edge[2]['weight'] = -waiting_time\n",
    "    b = np.max([-base_graph[x][y]['weight'] for x, y in base_match.items()]) # base max waiting time\n",
    "    base_match_size = len(base_match)\n",
    "    a = 0\n",
    "    \n",
    "    while a <= b :\n",
    "        lam = (b + a)//2\n",
    "        new_graph = copy.deepcopy(base_graph)\n",
    "        remove = [edge for edge in base_graph.edges(data = True) if -edge[2]['weight'] > lam]\n",
    "        new_graph.remove_edges_from(remove)\n",
    "        top_nodes = {n for n, d in new_graph.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "        new_match = nx.bipartite.maximum_matching(new_graph, top_nodes)\n",
    "        new_match_size = len(new_match)\n",
    "        wait_times = [-new_graph[x][y]['weight'] for x, y in new_match.items()]\n",
    "        # Every value larger than b gives the maximum cardinality matching\n",
    "        if base_match_size != new_match_size:\n",
    "            a = lam + 1\n",
    "        else:\n",
    "            b = lam - 1\n",
    "    # max wait time should be b + 1\n",
    "    remove = [edge for edge in base_graph.edges(data = True) if -edge[2]['weight'] > b + 1]\n",
    "    bottleneck_graph = copy.deepcopy(base_graph)\n",
    "    bottleneck_graph.remove_edges_from(remove)\n",
    "    top_nodes = {n for n, d in bottleneck_graph.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "    bottleneck_match = nx.bipartite.maximum_matching(new_graph, top_nodes)\n",
    "    return bottleneck_match, bottleneck_graph, b + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_match, bottleneck_graph, bottleneck_max_wait_time = bisection_search(B, match)\n",
    "print(f'Bottleneck max wait time is: {bottleneck_max_wait_time}mins')\n",
    "plot_weight_bipartite(B, bottleneck_match, match_to_path(bottleneck_match, trips), power = 0, with_labels = True, edge_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be also interested in knowing the max cardinality matching that minimizes the sum of the waiting time, or possibly even the power sum of the waiting time. This can be formulated as a min cost matching problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IP formulation of the min cost matching problem is shown as the following:\n",
    "$$\\begin{align*}\n",
    "\\min \\quad & \\sum_{e \\in E}c_ex_e \\\\\n",
    "\\text{s.t.} \\quad &  \\sum_{e \\in \\delta(v)} x_e \\leq 1 \\quad \\forall v \\in P \\cup D & (1)\\\\\n",
    "\\quad & x_e \\in \\{0,1\\} \\quad \\forall e \\in E & (2)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where, $\\delta(v)$ is the set of edges incident on the vertex $v \\in P \\cup D$, and $c_e$ is the edge weight （some power of the waiting time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cost_match(B, power):\n",
    "    '''Return a max card matching with the minimimum sum of the power of the waiting time'''\n",
    "    # Append weight (negative of the power of waiting time) to each edge\n",
    "    if power != 0:\n",
    "        for edge in B.edges(data = True):\n",
    "            waiting_time = edge[1][1] - edge[0][1] - times_df.at[(edge[0][0], edge[1][0])]\n",
    "            edge[2]['weight'] = -waiting_time **power\n",
    "        weight_match_tuple = nx.max_weight_matching(B, maxcardinality=True, weight='weight')\n",
    "        weight_match = dict()\n",
    "        for x, y in weight_match_tuple:\n",
    "            weight_match[x] = y\n",
    "            weight_match[y] = x\n",
    "        wait_times = [(-B[x][y]['weight'])**(1/power) for x, y in weight_match.items()]\n",
    "        total_wait_time = np.sum(wait_times)/2\n",
    "        max_wait_time = np.max(wait_times)\n",
    "#         print(f'total waiting time: {total_wait_time} mins')\n",
    "        print(f'maximum waiting time: {max_wait_time} mins')\n",
    "\n",
    "    else:\n",
    "        for edge in B.edges(data = True):\n",
    "            waiting_time = edge[1][1] - edge[0][1] - times_df.at[(edge[0][0], edge[1][0])]\n",
    "            edge[2]['weight'] = -waiting_time\n",
    "        weight_match = nx.bipartite.maximum_matching(B, top_nodes = {n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 0})\n",
    "        wait_times = [int(-B[x][y]['weight']) for x, y in weight_match.items()]\n",
    "        total_wait_time = np.sum(wait_times)/2\n",
    "        max_wait_time = np.max(wait_times)\n",
    "#         print('total waiting time: {0:.2f} mins'.format(total_wait_time))\n",
    "        print('maximum waiting time: {0:.2f} mins'.format(max_wait_time))\n",
    "    return weight_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimizing the total waiting time\n",
    "min_sum_match =  min_cost_match(B, 1)\n",
    "plot_weight_bipartite(B, min_sum_match, match_to_path(min_sum_match, trips), power = 1, with_labels = True, edge_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_square_sum_match =  min_cost_match(B, 2)\n",
    "plot_weight_bipartite(B, min_square_sum_match, match_to_path(min_square_sum_match, trips), power = 2, with_labels = True, edge_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_third_power_sum_match =  min_cost_match(B, 3)\n",
    "plot_weight_bipartite(B,min_third_power_sum_match, match_to_path(min_third_power_sum_match, trips), power = 3, with_labels = True, edge_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What do you notice about the maximum waiting time of the min-cost matching as we increase the power? Can you provide an explanation for this?\n",
    "\n",
    "**A:** <font color='blue'> The maximum waiting time of the min-cost matching converges to the bottleneck maximum waiting time as we increases the power. This is because increasing the power penalizes larger waiting time. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
