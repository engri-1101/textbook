{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import random\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import pickle\n",
    "\n",
    "# For the randomly-generated points\n",
    "NUM_POINTS = 500\n",
    "NUM_CLUSTERS = 5\n",
    "SQUARE_SIZE = 600\n",
    "NUM_OUTLIERS = 30\n",
    "\n",
    "# For the taxi data points\n",
    "NUM_TAXI_CLUSTERS = 5\n",
    "NUM_TAXI_OUTLIERS = 16\n",
    "\n",
    "TOLERANCE = ((SQUARE_SIZE**2/NUM_CLUSTERS)**0.5)\n",
    "print(TOLERANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4150a",
   "metadata": {},
   "source": [
    "The below cell generates a set of NUM_POINTS random points, to be split into NUM_CLUSTERS clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_dict = {}\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "np.random.seed(43)\n",
    "random_points = SQUARE_SIZE*np.random.rand(2,NUM_POINTS)\n",
    "for i in range(NUM_POINTS):\n",
    "    point_dict[(\"point_\"+str(i))] = (random_points[0][i],random_points[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in point_dict.keys():\n",
    "    temp_list = []\n",
    "    \n",
    "    for j in point_dict.keys():\n",
    "        dist_x = point_dict[i][0] - point_dict[j][0]\n",
    "        dist_y = point_dict[i][1] - point_dict[j][1]\n",
    "        distances_dict[(i,j)] = (dist_x**2+dist_y**2)**0.5\n",
    "        if (distances_dict[(i,j)] <= TOLERANCE):\n",
    "            temp_list.append(j)\n",
    "    possible_pairs[i] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5df091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plt.scatter(random_points[0],random_points[1],c=\"green\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formulate(possible_pairs, distances_dict, NUM_CLUSTERS, NUM_OUTLIERS=0):\n",
    "    m = gp.Model(\"clustering\")\n",
    "    pairs = {}\n",
    "    isCenter = {}\n",
    "    outlier = {}\n",
    "    for i in possible_pairs.keys():\n",
    "        isCenter[i] = m.addVar(vtype=GRB.BINARY, name = \"isCenter[%s]\" %i)\n",
    "        outlier[i] = m.addVar(vtype=GRB.BINARY, name = \"outlier[%s]\" %i)\n",
    "        for j in possible_pairs[i]:\n",
    "            pairs[i,j] = m.addVar(vtype=GRB.BINARY, name = \"pair{%s,%s}\" % (i,j))\n",
    "\n",
    "    for j in possible_pairs.keys():#constraint to define the isNotCenter variable\n",
    "        m.addConstr(isCenter[j] <= sum(pairs[i,j] for i in possible_pairs[j])) #isCenter[j] is LEQ than sum of all pairs [i,j]\n",
    "        for i in possible_pairs[j]:#to ensure that isCenter is 1 if any point has it as a center\n",
    "            m.addConstr(isCenter[j] - pairs[i,j] >= 0)\n",
    "\n",
    "    for i in possible_pairs.keys():\n",
    "        # this constraint was changed to ensure that each point i is either part of at least one cluster, or declared an outlier\n",
    "        m.addConstr(sum(pairs[i,j] for j in possible_pairs[i]) + outlier[i] >= 1) \n",
    "        m.addConstr(pairs[i,i] >= isCenter[i])\n",
    "\n",
    "    m.addConstr(sum(isCenter[j] for j in possible_pairs.keys()) == NUM_CLUSTERS)\n",
    "    m.addConstr(sum(outlier[i] for i in possible_pairs.keys()) <= NUM_OUTLIERS)\n",
    "\n",
    "    m.setObjective(sum(distances_dict[i,j]*pairs[i,j] for i in possible_pairs.keys() for j in possible_pairs[i]), GRB.MINIMIZE)\n",
    "    return m, pairs, isCenter, outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to display annotations on hover drew heavily from https://stackoverflow.com/a/47166787\n",
    "def plot_output(isCenter, pairs, outlier, point_dict, labels=None, mark_outliers=True):\n",
    "    if labels is None:\n",
    "        labels = dict(point_dict)\n",
    "        for label in labels:\n",
    "            labels[label] = f'({str(point_dict[label][0])[:6]}, {str(point_dict[label][1])[:6]})'\n",
    "            \n",
    "    centerDict = {}\n",
    "    for center in isCenter.keys():\n",
    "        if isCenter[center].x == 1:\n",
    "            centerDict[center] = []\n",
    "\n",
    "    for pair in pairs.keys():\n",
    "        if pairs[pair].x == 1:\n",
    "            centerDict[pair[1]].append(point_dict[pair[0]])\n",
    "\n",
    "    for center in centerDict.keys():\n",
    "        centerDict[center] = dict(centerDict[center])\n",
    "\n",
    "    outs = []\n",
    "    for out in outlier:\n",
    "        if outlier[out].x == 1:\n",
    "            outs.append(point_dict[out])\n",
    "            if mark_outliers:\n",
    "                labels[out] = 'Outlier: ' + labels[out]\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    c = []\n",
    "            \n",
    "    color_selection = list(range(len(centerDict)))\n",
    "    OUTLIER_COLOR=len(centerDict)\n",
    "    for i, center in enumerate(centerDict.keys()):\n",
    "        current_color = color_selection[i]\n",
    "        (keys,values) = zip(*centerDict[center].items())\n",
    "        x = x + list(keys)\n",
    "        y = y + list(values)\n",
    "        c = c + [current_color] * len(keys)\n",
    "    if len(outs) > 0:\n",
    "        keys, values = tuple(zip(*outs))\n",
    "        x = x + list(keys)\n",
    "        y = y + list(values)\n",
    "        c = c + [OUTLIER_COLOR] * len(keys)\n",
    "        \n",
    "    norm = plt.Normalize(0,len(centerDict))\n",
    "    cmap = plt.cm.coolwarm\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    sc = plt.scatter(x,y,c=c, cmap=cmap, norm=norm)\n",
    "    \n",
    "    annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\"))\n",
    "    \n",
    "    annot.set_visible(False)\n",
    "    annot.set_wrap(True)\n",
    "    \n",
    "    inv_map = {v: k for k, v in point_dict.items()}\n",
    "    \n",
    "    def update_annot(ind):\n",
    "        pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "        annot.xy = pos\n",
    "        text = str(labels[inv_map[tuple(pos)]])\n",
    "        \n",
    "        annot.set_text(text)\n",
    "        annot.get_bbox_patch().set_facecolor(cmap(norm(c[ind[\"ind\"][0]])))\n",
    "\n",
    "\n",
    "    def hover(event):\n",
    "        vis = annot.get_visible()\n",
    "        if event.inaxes == ax:\n",
    "            cont, ind = sc.contains(event)\n",
    "            if cont:\n",
    "                update_annot(ind)\n",
    "                annot.set_visible(True)\n",
    "                fig.canvas.draw_idle()\n",
    "            else:\n",
    "                if vis:\n",
    "                    annot.set_visible(False)\n",
    "                    fig.canvas.draw_idle()\n",
    "                    \n",
    "    fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulation without outliers\n",
    "m, pairs, isCenter, outlier = formulate(possible_pairs, distances_dict, NUM_CLUSTERS)\n",
    "# Uncomment the line below to suppress Gurobi's output\n",
    "# m.Params.LogToConsole = 0\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(isCenter, pairs, outlier, point_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59caf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the k-medians problem while allowing NUM_OUTLIERS points to go un-classified\n",
    "m, pairs, isCenter, outlier = formulate(possible_pairs, distances_dict, NUM_CLUSTERS, NUM_OUTLIERS)\n",
    "# Uncomment the line below to suppress Gurobi's output\n",
    "# m.Params.LogToConsole = 0\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa987ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(isCenter, pairs, outlier, point_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, trying the above code on the taxi data.\n",
    "# Locally, I copied the taxi_count_dict.pickle file from the MST clustering lab.\n",
    "# These are number of rides hailed in 15? minute intervals for every day of one year.\n",
    "# See the other lab for more info I guess\n",
    "\n",
    "with open('data/taxi_count_dict.pickle', 'rb') as handle:\n",
    "    taxi_counts = pd.DataFrame(pickle.load(handle))\n",
    "print(taxi_counts.loc[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(taxi_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data again\n",
    "point_dict = {}\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "for i in range(len(taxi_counts)):\n",
    "    point_dict[(\"day_\"+str(i))] = taxi_counts.loc[i][\"count_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in point_dict.keys():\n",
    "    temp_list = []\n",
    "    for j in point_dict.keys():\n",
    "        distances_dict[(i,j)] = np.linalg.norm(np.array(point_dict[i]) - np.array(point_dict[j]), ord=1)\n",
    "        if True:#(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "    possible_pairs[i] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering for the taxi dataset\n",
    "NUM_OUTLIERS = NUM_TAXI_OUTLIERS\n",
    "NUM_CLUSTERS = NUM_TAXI_CLUSTERS\n",
    "\n",
    "m, pairs, isCenter, outlier = formulate(possible_pairs, distances_dict, NUM_CLUSTERS, NUM_OUTLIERS)\n",
    "# Uncomment the line below to suppress Gurobi's output\n",
    "# m.Params.LogToConsole = 0\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The above cell took ~ 15 seconds for me to run. \n",
    "# It may be worth seeing how long it takes if re-implemented in or-tools with their free MIP solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centerDict = {}\n",
    "for center in isCenter.keys():\n",
    "    if isCenter[center].x == 1:\n",
    "        centerDict[center] = []\n",
    "        \n",
    "for pair in pairs.keys():\n",
    "    if pairs[pair].x == 1:\n",
    "        centerDict[pair[1]].append(pair[0])\n",
    "        \n",
    "outs = []\n",
    "for out in outlier:\n",
    "    if outlier[out].x == 1:\n",
    "        outs.append(out)\n",
    "        \n",
    "# A little bit to try to visualize the output.\n",
    "# Indicates information about each of the outliers (month,day,day-of-week)\n",
    "# and the same information about all the days, grouped by clusters.\n",
    "    \n",
    "print(\"OUTLIERS\")\n",
    "for day in outs:\n",
    "    entry = taxi_counts.loc[int(day[4:])]\n",
    "    print(day, \"%s,%s:  %s\" % (entry[\"m\"],entry[\"d\"],entry[\"weekday\"]))\n",
    "\n",
    "for center in centerDict:\n",
    "    print(\"\")\n",
    "    print(\"CLUSTER\")\n",
    "    for day in centerDict[center]:\n",
    "        entry = taxi_counts.loc[int(day[4:])]\n",
    "        print(day, \"%s,%s:  %s\" % (entry[\"m\"],entry[\"d\"],entry[\"weekday\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization.\n",
    "# We want to embed the high-dimensional points into a 2-dimensional space so we can do a scatter plot.\n",
    "\n",
    "# Here is an outline of what hellinger_manifold_mds does:\n",
    "# - treat the points as probability distributions (by normalizing)\n",
    "# - find the pairwise hellinger distance between them\n",
    "# - use \"multidimensional scaling\" to embed into two dimensions (See https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)\n",
    "\n",
    "# Possible steps \n",
    "# - Use some other standard clustering metrics to compare this clustering result against the MST clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255868ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the Hellinger distances of iterables of numbers x and y\n",
    "def hellinger(x, y):\n",
    "    # Normalize x, y\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    for a in (x,y):\n",
    "        s = sum(a)\n",
    "        for i in range(len(a)):\n",
    "            a[i] = a[i] / s\n",
    "    # Compute Hellinger distance\n",
    "    squares = []\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        squares.append((math.sqrt(x_i) - math.sqrt(y_i)) ** 2)\n",
    "    return sum(squares) / math.sqrt(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization method using sklearn.manifold.MDS with Hellinger distance\n",
    "# Uses sklearn's implementation of the SMACOF algorithm to scale point_dict to two dimensions using \n",
    "# the Hellinger distance between points\n",
    "def hellinger_manifold_mds(point_dict, distances_dict):\n",
    "    # Set up the hellinger distance matrix\n",
    "    n = len(point_dict)\n",
    "    D = np.zeros((n,n))\n",
    "    for tup in distances_dict.keys():\n",
    "        i = int(tup[0][len('day_'):])\n",
    "        j = int(tup[1][len('day_'):])\n",
    "        D[i,j] = hellinger(point_dict[tup[0]], point_dict[tup[1]])\n",
    "    mds = MDS(dissimilarity='precomputed')\n",
    "    return mds.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14196bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization method using sklearn.manifold.MDS with Euclidean distance\n",
    "# Uses sklearn's implementation of the SMACOF algorithm to scale point_dict to two dimensions using \n",
    "# the Euclidean distance between points\n",
    "def euclidean_manifold_mds(point_dict, distances_dict):\n",
    "    X = np.row_stack(tuple(np.array(point_dict[i]) for i in point_dict))\n",
    "    mds = MDS()\n",
    "    return mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a987b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate visualization method\n",
    "# Implements the classical MDS algorithm outlined in https://en.wikipedia.org/wiki/Multidimensional_scaling\n",
    "def classical_mds(point_dict, distances_dict):\n",
    "    DIM = 2 # The dimension to scale down to\n",
    "    n = len(point_dict)\n",
    "    # 1: Set up the squared proximity matrix\n",
    "    D = np.zeros((n,n))\n",
    "    for tup in distances_dict.keys():\n",
    "        i = int(tup[0][len('day_'):])\n",
    "        j = int(tup[1][len('day_'):])\n",
    "        D[i,j] = distances_dict[tup]**2\n",
    "\n",
    "    # 2: Apply double-centering\n",
    "    C = np.identity(n) - 1/n * np.ones((n,n))\n",
    "    B = -1/2 * C.dot(D).dot(C)\n",
    "\n",
    "    # 3: Get the DIM largest eigenvalues and corresponding eigenvectors\n",
    "    w, v = np.linalg.eig(B)\n",
    "    dim_largest = sorted(w, reverse=True)[:DIM]\n",
    "    wlist = w.tolist()\n",
    "    w_indices = [wlist.index(i) for i in dim_largest]\n",
    "\n",
    "    # 4: Get the new coordinates matrix, X\n",
    "    return np.column_stack(tuple(v[:,i] for i in w_indices)).dot(np.diag(np.array([i**(1/2) for i in dim_largest])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ff969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the MDS algorithm here. Uncomment one of the following\n",
    "# X = hellinger_manifold_mds(point_dict, distances_dict)\n",
    "X = euclidean_manifold_mds(point_dict, distances_dict)\n",
    "# X = classical_mds(point_dict, distances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe73dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override each element of point_dict with the dimension change\n",
    "new_point_dict = point_dict.copy()\n",
    "for i in new_point_dict:\n",
    "    j = int(i[len('day_'):])\n",
    "    new_point_dict[i] = tuple(X[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "for i in new_point_dict:\n",
    "    j = int(i[len('day_'):])\n",
    "    entry = taxi_counts.loc[j]\n",
    "    month = entry[\"m\"]\n",
    "    day = entry[\"d\"]\n",
    "    weekday = entry[\"weekday\"]\n",
    "    labels[i] = f'{days[weekday]}, {months[month-1]} {day}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering plot\n",
    "plot_output(isCenter, pairs, outlier, new_point_dict, dict(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot colored by weekdays\n",
    "class xObj:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "days = {i: None for i in range(7)}\n",
    "isCenter = {}\n",
    "pairs = {}\n",
    "for i in new_point_dict:\n",
    "    j = int(i[len('day_'):])\n",
    "    entry = taxi_counts.loc[j]\n",
    "    month = entry[\"m\"]\n",
    "    day = entry[\"d\"]\n",
    "    weekday = entry[\"weekday\"]\n",
    "    x = new_point_dict[i][0]\n",
    "    y = new_point_dict[i][1]\n",
    "    if days[weekday] is None:\n",
    "        days[weekday] = i\n",
    "        isCenter[i] = xObj(1)\n",
    "    else:\n",
    "        isCenter[i] = xObj(0)\n",
    "    for k in days:\n",
    "        if days[k] is None:\n",
    "            continue\n",
    "        pair = (i, days[k])\n",
    "        val = 1 if k == weekday else 0\n",
    "        pairs[pair] = xObj(val)\n",
    "\n",
    "plot_output(isCenter, pairs, {}, new_point_dict, dict(labels))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
