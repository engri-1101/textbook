{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ortools.linear_solver import pywraplp as OR\n",
    "import random\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import pickle\n",
    "\n",
    "COLOR_LIST = list(mcolors.CSS4_COLORS.keys())\n",
    "NUM_POINTS = 500\n",
    "NUM_CLUSTERS = 5\n",
    "SQUARE_SIZE = 600\n",
    "\n",
    "TOLERANCE = ((SQUARE_SIZE**2/NUM_CLUSTERS)**0.5)\n",
    "print(TOLERANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4150a",
   "metadata": {},
   "source": [
    "The below cell generates a set of 1000 random points, to be split into 20 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0c0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_dict = {}\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "random_points = SQUARE_SIZE*np.random.rand(2,NUM_POINTS)\n",
    "for i in range(NUM_POINTS):\n",
    "    point_dict[(\"point_\"+str(i))] = (random_points[0][i],random_points[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in point_dict.keys():\n",
    "    temp_list = []\n",
    "    \n",
    "    for j in point_dict.keys():\n",
    "        dist_x = point_dict[i][0] - point_dict[j][0]\n",
    "        dist_y = point_dict[i][1] - point_dict[j][1]\n",
    "        distances_dict[(i,j)] = (dist_x**2+dist_y**2)**0.5\n",
    "        if (distances_dict[(i,j)] <= TOLERANCE):\n",
    "            temp_list.append(j)\n",
    "    possible_pairs[i] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5df091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plt.scatter(random_points[0],random_points[1],c=\"green\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard k-medians implementation.\n",
    "\n",
    "m = gp.Model(\"clustering\")\n",
    "pairs = {}\n",
    "isCenter = {}\n",
    "for i in possible_pairs.keys():\n",
    "    isCenter[i] = m.addVar(vtype=GRB.BINARY, name = \"isCenter[%s]\" %i)\n",
    "    for j in possible_pairs[i]:\n",
    "        pairs[i,j] = m.addVar(vtype=GRB.BINARY, name = \"pair{%s,%s}\" % (i,j))\n",
    "        \n",
    "for j in possible_pairs.keys():#constraint to define the isNotCenter variable\n",
    "    m.addConstr(isCenter[j] <= sum(pairs[i,j] for i in possible_pairs[j])) #isCenter[j] is LEQ than sum of all pairs [i,j]\n",
    "    for i in possible_pairs[j]:#to ensure that isCenter is 1 if any point has it as a center\n",
    "        m.addConstr(isCenter[j] - pairs[i,j] >= 0)\n",
    "\n",
    "for i in possible_pairs.keys(): #constraint to ensure that every point has 1 associated cluster\n",
    "    m.addConstr(sum(pairs[i,j] for j in possible_pairs[i]) == 1)\n",
    "    m.addConstr(pairs[i,i] >= isCenter[i])\n",
    "    \n",
    "m.addConstr(sum(isCenter[j] for j in possible_pairs.keys()) == NUM_CLUSTERS)\n",
    "    \n",
    "m.setObjective(sum(distances_dict[i,j]*pairs[i,j] for i in possible_pairs.keys() for j in possible_pairs[i]), GRB.MINIMIZE)\n",
    "m.optimize()\n",
    "\n",
    "# for v in m.getVars():\n",
    "#     print('%s %g' % (v.varName, v.x))\n",
    "# print('Obj: %g' % m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de058de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the k-medians problem while allowing 30 points to go un-classified\n",
    "\n",
    "NUM_OUTLIERS = 30\n",
    "\n",
    "m = gp.Model(\"clustering\")\n",
    "pairs = {}\n",
    "isCenter = {}\n",
    "outlier = {}\n",
    "for i in possible_pairs.keys():\n",
    "    isCenter[i] = m.addVar(vtype=GRB.BINARY, name = \"isCenter[%s]\" %i)\n",
    "    outlier[i] = m.addVar(vtype=GRB.BINARY, name = \"outlier[%s]\" %i)\n",
    "    for j in possible_pairs[i]:\n",
    "        pairs[i,j] = m.addVar(vtype=GRB.BINARY, name = \"pair{%s,%s}\" % (i,j))\n",
    "        \n",
    "for j in possible_pairs.keys():#constraint to define the isNotCenter variable\n",
    "    m.addConstr(isCenter[j] <= sum(pairs[i,j] for i in possible_pairs[j])) #isCenter[j] is LEQ than sum of all pairs [i,j]\n",
    "    for i in possible_pairs[j]:#to ensure that isCenter is 1 if any point has it as a center\n",
    "        m.addConstr(isCenter[j] - pairs[i,j] >= 0)\n",
    "\n",
    "for i in possible_pairs.keys():\n",
    "    # this constraint was changed to ensure that each point i is either part of at least one cluster, or declared an outlier\n",
    "    m.addConstr(sum(pairs[i,j] for j in possible_pairs[i]) + outlier[i] >= 1) \n",
    "    m.addConstr(pairs[i,i] >= isCenter[i])\n",
    "    \n",
    "m.addConstr(sum(isCenter[j] for j in possible_pairs.keys()) == NUM_CLUSTERS)\n",
    "m.addConstr(sum(outlier[i] for i in possible_pairs.keys()) <= NUM_OUTLIERS)\n",
    "    \n",
    "m.setObjective(sum(distances_dict[i,j]*pairs[i,j] for i in possible_pairs.keys() for j in possible_pairs[i]), GRB.MINIMIZE)\n",
    "m.optimize()\n",
    "\n",
    "# for v in m.getVars():\n",
    "#     print('%s %g' % (v.varName, v.x))\n",
    "# print('Obj: %g' % m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "centerDict = {}\n",
    "for center in isCenter.keys():\n",
    "    if isCenter[center].x == 1:\n",
    "        centerDict[center] = []\n",
    "        \n",
    "for pair in pairs.keys():\n",
    "    if pairs[pair].x == 1:\n",
    "        centerDict[pair[1]].append(point_dict[pair[0]])\n",
    "\n",
    "for center in centerDict.keys():\n",
    "    centerDict[center] = dict(centerDict[center])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for out in outlier:\n",
    "    if outlier[out].x == 1:\n",
    "        outs.append(point_dict[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa987ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_selection = np.random.choice(COLOR_LIST,NUM_CLUSTERS)\n",
    "for center in centerDict.keys():\n",
    "    current_color = np.random.rand(1,3)\n",
    "    (keys,values) = zip(*centerDict[center].items())\n",
    "    plt.plot(keys,values,c=current_color)\n",
    "    plt.scatter(keys,values,c=current_color)\n",
    "plt.scatter(*zip(*outs),c=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, trying the above code on the taxi data.\n",
    "# Locally, I copied the taxi_count_dict.pickle file from the MST clustering lab.\n",
    "# These are number of rides hailed in 15? minute intervals for every day of one year.\n",
    "# See the other lab for more info I guess\n",
    "\n",
    "with open('data/taxi_count_dict.pickle', 'rb') as handle:\n",
    "    taxi_counts = pd.DataFrame(pickle.load(handle))\n",
    "print(taxi_counts.loc[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(taxi_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data again\n",
    "point_dict = {}\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "for i in range(len(taxi_counts)):\n",
    "    point_dict[(\"day_\"+str(i))] = taxi_counts.loc[i][\"count_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in point_dict.keys():\n",
    "    temp_list = []\n",
    "    \n",
    "    for j in point_dict.keys():\n",
    "        distances_dict[(i,j)] = np.linalg.norm(np.array(point_dict[i]) - np.array(point_dict[j]), ord=1)\n",
    "        if True:#(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "    possible_pairs[i] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-pasted from above. TODO: put in a single function\n",
    "NUM_OUTLIERS = 12\n",
    "NUM_CLUSTERS = 4\n",
    "\n",
    "m = gp.Model(\"clustering\")\n",
    "pairs = {}\n",
    "isCenter = {}\n",
    "outlier = {}\n",
    "for i in possible_pairs.keys():\n",
    "    isCenter[i] = m.addVar(vtype=GRB.BINARY, name = \"isCenter[%s]\" %i)\n",
    "    outlier[i] = m.addVar(vtype=GRB.BINARY, name = \"outlier[%s]\" %i)\n",
    "    for j in possible_pairs[i]:\n",
    "        pairs[i,j] = m.addVar(vtype=GRB.BINARY, name = \"pair{%s,%s}\" % (i,j))\n",
    "        \n",
    "for j in possible_pairs.keys():#constraint to define the isNotCenter variable\n",
    "    m.addConstr(isCenter[j] <= sum(pairs[i,j] for i in possible_pairs[j])) #isCenter[j] is LEQ than sum of all pairs [i,j]\n",
    "    for i in possible_pairs[j]:#to ensure that isCenter is 1 if any point has it as a center\n",
    "        m.addConstr(isCenter[j] - pairs[i,j] >= 0)\n",
    "\n",
    "for i in possible_pairs.keys(): #constraint to ensure that every point has 1 associated cluster\n",
    "    m.addConstr(sum(pairs[i,j] for j in possible_pairs[i]) + outlier[i] >= 1)\n",
    "    m.addConstr(pairs[i,i] >= isCenter[i])\n",
    "    \n",
    "m.addConstr(sum(isCenter[j] for j in possible_pairs.keys()) == NUM_CLUSTERS)\n",
    "m.addConstr(sum(outlier[i] for i in possible_pairs.keys()) <= NUM_OUTLIERS)\n",
    "    \n",
    "m.setObjective(sum(distances_dict[i,j]*pairs[i,j] for i in possible_pairs.keys() for j in possible_pairs[i]), GRB.MINIMIZE)\n",
    "m.optimize()\n",
    "\n",
    "# for v in m.getVars():\n",
    "#     print('%s %g' % (v.varName, v.x))\n",
    "# print('Obj: %g' % m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The above cell took ~ 15 seconds for me to run. \n",
    "# It may be worth seeing how long it takes if re-implemented in or-tools with their free MIP solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centerDict = {}\n",
    "for center in isCenter.keys():\n",
    "    if isCenter[center].x == 1:\n",
    "        centerDict[center] = []\n",
    "        \n",
    "for pair in pairs.keys():\n",
    "    if pairs[pair].x == 1:\n",
    "        centerDict[pair[1]].append(pair[0])\n",
    "        \n",
    "outs = []\n",
    "for out in outlier:\n",
    "    if outlier[out].x == 1:\n",
    "        outs.append(out)\n",
    "        \n",
    "# A little bit to try to visualize the output.\n",
    "# Indicates information about each of the outliers (month,day,day-of-week)\n",
    "# and the same information about all the days, grouped by clusters.\n",
    "    \n",
    "print(\"OUTLIERS\")\n",
    "for day in outs:\n",
    "    entry = taxi_counts.loc[int(day[4:])]\n",
    "    print(day, \"%s,%s:  %s\" % (entry[\"m\"],entry[\"d\"],entry[\"weekday\"]))\n",
    "\n",
    "for center in centerDict:\n",
    "    print(\"\")\n",
    "    print(\"CLUSTER\")\n",
    "    for day in centerDict[center]:\n",
    "        entry = taxi_counts.loc[int(day[4:])]\n",
    "        print(day, \"%s,%s:  %s\" % (entry[\"m\"],entry[\"d\"],entry[\"weekday\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization.\n",
    "# Below is NOT working atm\n",
    "# We want to embed the high-dimensional points into a 2-dimensional space so we can do a scatter plot.\n",
    "\n",
    "# This code is copied from Vivek from some urban analytics class \n",
    "# def embed():\n",
    "#     '''Generate 2D embedding by hellinger distance of 24D global \"points\" and store in global\n",
    "#     \"em\". The computed distances and 24D vectors are also stored globally. The SMACOF algorithm \n",
    "#     is used for the projection.\n",
    "#     '''\n",
    "#     global vecs\n",
    "#     global distances\n",
    "#     global em\n",
    "#     shuffle(points)\n",
    "#     # Use normalized vectors\n",
    "#     vecs = [norm(i.vec) for i in points]\n",
    "#     distances = [[hellinger(a, b) for a in vecs] for b in tqdm(vecs)]\n",
    "\n",
    "#     mds = manifold.MDS(\n",
    "#         n_components=2, \n",
    "#         max_iter=300,\n",
    "#         eps=1e-9,\n",
    "#         random_state=np.random.RandomState(seed=0), \n",
    "#         dissimilarity=\"precomputed\", \n",
    "#         n_jobs=1\n",
    "#     )\n",
    "    \n",
    "#     em = mds.fit(distances).embedding_\n",
    "\n",
    "# Here is an outline of what the above does:\n",
    "# - treat the points as probability distributions (by normalizing)\n",
    "# - find the pairwise hellinger distance between them\n",
    "# - use \"multidimensional scaling\" to embed into two dimensions (See https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)\n",
    "\n",
    "\n",
    "# Problems:\n",
    "# - I don't think hellinger distance is appropriate\n",
    "#   - In particular, we shouldn't normalize the data (unless we also want to normalize before clustering)\n",
    "# - The MDS requires sklearn, which is very big. We can use this to generate a plot, but I don't think\n",
    "#     running this part is appropriate for student use. \n",
    "\n",
    "# Possible steps \n",
    "# - Use some other standard clustering metrics to compare this clustering result against the MST clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a987b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
